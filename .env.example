# Example environment configuration for py-toon benchmarks
# Copy this file to .env and fill in your values

# ============================================================================
# LLM Accuracy Benchmark Configuration
# ============================================================================

# Provider Selection (choose one)
# Options: openai, vertex
# PROVIDER=openai

# ----------------------------------------------------------------------------
# OpenAI Configuration
# ----------------------------------------------------------------------------
# Dual API keys enable separate tracking in OpenAI console for JSON vs TOON
# evaluations. You can use the same key for both if separate tracking isn't needed.
# OPENAI_API_KEY_JSON=sk-...
# OPENAI_API_KEY_TOON=sk-...

# OpenAI model selection (optional, defaults to gpt-4o)
# OPENAI_MODEL=gpt-4o
# VALIDATION_MODEL=gpt-4o-mini

# ----------------------------------------------------------------------------
# Google Vertex AI Configuration (Recommended for Free Credits)
# ----------------------------------------------------------------------------
# Your Google Cloud Project ID
# Get from: https://console.cloud.google.com/
VERTEX_PROJECT_ID=your-project-id

# GCP region for Vertex AI API calls
# Common regions: us-central1, us-east1, europe-west1, asia-southeast1
# Default: us-central1
VERTEX_LOCATION=us-central1

# Authentication: Use JSON service account key file (recommended)
# Create service account key at: https://console.cloud.google.com/iam-admin/serviceaccounts
# Download JSON key and set path here:
GOOGLE_APPLICATION_CREDENTIALS=/path/to/your-service-account-key.json

# Alternative: Use Application Default Credentials (ADC)
# If GOOGLE_APPLICATION_CREDENTIALS is not set, falls back to ADC:
#   1. Install gcloud CLI: https://cloud.google.com/sdk/docs/install
#   2. Run: gcloud auth application-default login

# Model selection when using Vertex AI (Latest Gemini 2.5 series - Oct 2025)
# Default configuration (optimized for cost):
#   - Benchmark: gpt-5-mini -> gemini-2.5-flash (fastest, cheapest)
#   - Validation: gpt-5 -> gemini-2.5-pro (best quality)
# 
# The benchmark automatically maps model names to latest Vertex AI models:
#   gpt-5-mini, gpt-4o-mini, gpt-3.5-turbo -> gemini-2.5-flash
#   gpt-5, gpt-4, gpt-4o -> gemini-2.5-pro
# 
# You can also specify Vertex AI models directly:
#   OPENAI_MODEL=gemini-2.5-flash         # For benchmark (cost-optimized)
#   VALIDATION_MODEL=gemini-2.5-pro       # For validation (quality-optimized)

# ----------------------------------------------------------------------------
# Benchmark Execution Settings
# ----------------------------------------------------------------------------

# Parallel evaluation tasks (higher = faster but more API load)
# Default: 20
# CONCURRENCY=20

# Dry run mode: limit to 10 questions for cost control
# Set to 'true' to enable, 'false' or unset to disable
# DRY_RUN=false

# Keep last N timestamped result sets for historical tracking
# Set to 0 to disable cleanup
# Default: 10
# KEEP_LAST_N_RUNS=10
