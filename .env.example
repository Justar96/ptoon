# OpenAI API Keys for LLM Accuracy Benchmark
# =============================================
# To run the LLM accuracy benchmark, you need TWO separate OpenAI API keys.
# This enables independent usage tracking in your OpenAI dashboard.

# JSON format evaluation API key
OPENAI_API_KEY_JSON=sk-proj-your-json-api-key-here

# TOON format evaluation API key
OPENAI_API_KEY_TOON=sk-proj-your-toon-api-key-here

# Optional: Separate API key for validation (LLM-as-judge)
# If not set, validation will use OPENAI_API_KEY_JSON
# OPENAI_API_KEY_VALIDATION=sk-proj-your-validation-api-key-here

# Optional: Override the model used for evaluation
# Default: gpt-5
# OPENAI_MODEL=gpt-5

# Optional: Override the model used for validation
# Default: gpt-4o-mini
# VALIDATION_MODEL=gpt-4o-mini

# Optional: Override pricing constants (per 1M tokens)
# These defaults match GPT-5 pricing as of August 2025
# OPENAI_PRICE_INPUT_PER_1M=1.25
# OPENAI_PRICE_OUTPUT_PER_1M=10.00
# OPENAI_PRICE_CACHED_INPUT_PER_1M=0.125

# Setup Instructions:
# ===================
# 1. Copy this file to .env: cp .env.example .env
# 2. Get your OpenAI API keys from: https://platform.openai.com/api-keys
# 3. Create two separate keys (for independent tracking)
# 4. Replace the placeholder values above
# 5. Never commit .env to git (it's in .gitignore)

# Security Note:
# ==============
# If you accidentally commit API keys:
# 1. Immediately revoke them in OpenAI dashboard
# 2. Generate new keys
# 3. Remove from git history: git filter-branch or BFG Repo-Cleaner
