# OpenAI API Key (required for openai_integration.py)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Optional: Override default model (default: gpt-4o-mini)
# OPENAI_MODEL=gpt-4o-mini

# Optional: Override default temperature (default: 0 for deterministic responses)
# OPENAI_TEMPERATURE=0

# Optional: Enable verbose logging
# VERBOSE=true

# ============================================================================
# LLM Accuracy Benchmark Configuration
# ============================================================================
# The LLM accuracy benchmark compares JSON vs TOON format accuracy using
# dual API keys for independent tracking in the OpenAI console.
#
# Run with: uv run toon-llm-benchmark
# Or: python -m benchmarks.llm_accuracy

# Required: Dual OpenAI API keys for separate tracking
# These allow you to monitor JSON and TOON evaluations independently
# in your OpenAI usage dashboard for cost and performance analysis.
OPENAI_API_KEY_JSON=your-openai-api-key-for-json-evaluation
OPENAI_API_KEY_TOON=your-openai-api-key-for-toon-evaluation

# Optional: Enable dry-run mode (limits to 10 questions for cost control)
# Useful for testing the benchmark without incurring high API costs.
# Can be overridden with --dry-run CLI flag.
# DRY_RUN=true

# Optional: Parallel evaluation concurrency (default: 20)
# Higher values = faster completion but more concurrent API calls.
# Lower values = slower but more conservative rate limiting.
# Can be overridden with --concurrency CLI flag.
# CONCURRENCY=20

# Optional: Enable verbose logging for debugging
# Shows detailed progress, API calls, and validation results.
# Can be enabled with --verbose CLI flag.
# VERBOSE=true
