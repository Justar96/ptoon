### LLM Accuracy Benchmark Results
Tested with **gemini-2.5-flash** across **162 questions**

```
JSON         ░░░░░░░░░░░░░░░░░░░░    0.0% (0/162)
TOON         ░░░░░░░░░░░░░░░░░░░░    0.0% (0/162)
```

**Advantage:** TOON achieves **0.0% accuracy** (vs JSON's 0.0%) while using **48.5% fewer tokens**.

| Format | Accuracy | Avg Tokens | Avg Latency (ms) | Input Tokens | Output Tokens | Est. Cost |
|--------|----------|------------|------------------|--------------|---------------|-----------|
| `JSON` | 0.0% | 11,076 | 3823.0 | 1,800,853 | 522 | $0.5416 |
| `TOON` | 0.0% | 5,703 | 4155.6 | 898,639 | 492 | $0.2708 |

*Costs based on Google Vertex AI pricing: $0.30 per 1M input tokens, $2.50 per 1M output tokens (cached: $0.030/1M). Estimates use regular pricing; actual costs may be ~40-60% lower with prompt caching.*

| Dataset | JSON Tokens | TOON Tokens | Savings | Bar |
|---------|-------------|-------------|---------|-----|
| Uniform employee records (TOON optimal format) | 5,992 | 2,162 | 63.9% | `███████░░░░░░░░░░░░░ 63.9% saved` |
| E-commerce orders with nested structures | 10,654 | 7,046 | 33.9% | `█████████████░░░░░░░ 33.9% saved` |
| Time-series analytics data | 10,969 | 4,499 | 59.0% | `████████░░░░░░░░░░░░ 59.0% saved` |
| Top 100 GitHub repositories | 16,688 | 9,106 | 45.4% | `███████████░░░░░░░░░ 45.4% saved` |

<details>
<summary><strong>View detailed breakdown by dataset</strong></summary>

#### Performance by Dataset

##### Uniform employee records (TOON optimal format)

| Format | Accuracy | Tokens | Correct/Total |
|--------|----------|--------|---------------|
| `JSON` | 0.0% | 5,992 | 0/55 |
| `TOON` | 0.0% | 2,162 | 0/55 |

##### E-commerce orders with nested structures

| Format | Accuracy | Tokens | Correct/Total |
|--------|----------|--------|---------------|
| `JSON` | 0.0% | 10,654 | 0/40 |
| `TOON` | 0.0% | 7,046 | 0/40 |

##### Time-series analytics data

| Format | Accuracy | Tokens | Correct/Total |
|--------|----------|--------|---------------|
| `JSON` | 0.0% | 10,969 | 0/37 |
| `TOON` | 0.0% | 4,499 | 0/37 |

##### Top 100 GitHub repositories

| Format | Accuracy | Tokens | Correct/Total |
|--------|----------|--------|---------------|
| `JSON` | 0.0% | 16,688 | 0/30 |
| `TOON` | 0.0% | 9,106 | 0/30 |

#### Methodology

- **Semantic validation**: LLM-as-judge validates responses semantically (not exact string matching).
- **Token counting**: Using `tiktoken` with `o200k_base` encoding (equivalent to gpt-tokenizer).
- **Question types**: 162 questions across field retrieval, aggregation, and filtering tasks.
- **Datasets**: Faker-generated datasets (seeded for reproducibility) + GitHub repositories.
- **Model**: gemini-2.5-flash
- **Dual API keys**: Separate OpenAI API keys used for JSON and TOON evaluations to enable independent tracking.

</details>